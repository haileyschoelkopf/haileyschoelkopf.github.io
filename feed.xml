<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://haileyschoelkopf.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://haileyschoelkopf.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-13T13:14:39+00:00</updated><id>https://haileyschoelkopf.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Prefix Linear Attention Can Outspeed Causal Linear Attention</title><link href="https://haileyschoelkopf.github.io/blog/2024/prefix-lm/" rel="alternate" type="text/html" title="Prefix Linear Attention Can Outspeed Causal Linear Attention"/><published>2024-08-11T00:00:00+00:00</published><updated>2024-08-11T00:00:00+00:00</updated><id>https://haileyschoelkopf.github.io/blog/2024/prefix-lm</id><content type="html" xml:base="https://haileyschoelkopf.github.io/blog/2024/prefix-lm/"><![CDATA[<p><em>This blog post describes 1) my opinions on Prefix Language Modeling objectives for Transformer-based decoder-only language models, and then 2) describes why this is not the case for Linear Attention-style recurrent LM architectures–PrefixLM can actually be faster than Causal LM for these architectures in some settings! Part 2) is inspired by the recent JRT-RNN paper which applies PrefixLM to Linear Attention-based recurrent models, but states a fact about extra added efficiency I could not see written out anywhere in that paper.</em></p> <p><em>I’ve not gotten around to actually trying to write a more efficient impl. of prefixLM linear attention using this fact, and the class of architectures for which this should help is limited. I just wanted to write down these two observations somewhere less ephemeral.</em></p> <p><em>Prerequisites: If you are unfamiliar with Linear Attention, it may help to read about the basic ideas of what it is and how it’s computed in <a href="https://haileyschoelkopf.github.io/blog/2024/linear-attn/">my previous post</a> or the papers linked in that post. I try to cover all necessary background very briefly in this post, however.</em></p> <h2 id="introduction">Introduction</h2> <p>Most current language models are trained using a Causal Language Modeling (“Causal LM”) objective. This means that they predict 1 token at a time, left-to-right.</p> <p>There are other variants that have been used or proposed, ranging from the original Masked Language Modeling (MLM<d-cite key="devlin-etal-2019-bert"></d-cite>) or the more general T5-style Span Corruption<d-cite key="raffel2023exploringlimitstransferlearning"></d-cite>, hybrids of causal language modeling and span corruption (Fill-In-The-Middle<d-cite key="bavarian2022efficienttraininglanguagemodels"></d-cite>, GLM<d-cite key="du-etal-2022-glm"></d-cite>), mixtures of these objectives (UL2<d-cite key="tay2023ul2unifyinglanguagelearning"></d-cite>, UL2R<d-cite key="tay2022transcendingscalinglaws01"></d-cite>), and more.</p> <p>The one we’ll focus on in this post is <strong>Prefix Language Modeling (“PrefixLM”)</strong>. See the diagram below for an image demonstrating the difference between PrefixLM and Causal LM: in PrefixLM, some initial subset of tokens may attend bidirectionally to each other (the “prefix”), followed by all other tokens being produced autogressively as in Causal LM. For more details on the other objectives, my favorite treatment of these is an in-depth study by Wang et al. (2022)<d-cite key="pmlr-v162-wang22u"></d-cite><d-footnote>I can't recommend this paper highly enough, please read it!</d-footnote> that attempts to disentangle the role of these objectives and the architectures often associated with them, based on zero-shot performance. Others have also written well about language modeling objectives<d-cite key="tay2024objectives"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prefix-lm/noncausal-decoder-diagram-480.webp 480w,/assets/img/prefix-lm/noncausal-decoder-diagram-800.webp 800w,/assets/img/prefix-lm/noncausal-decoder-diagram-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prefix-lm/noncausal-decoder-diagram.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Diagram showing the differences in attention pattern between Causal Language Modeling ("Causal Decoder", Left) and Prefix Language Modeling ("Noncausal Decoder", Right). In Prefix Language Modeling, a set amount of tokens at the beginning of the input (here, the string "I am") are allowed to attend bidirectionally to each other, called the "prefix".</figcaption> </figure> <p>I have personally <a href="https://twitter.com/haileysch__/status/1691483230761857024?s=46">not been a fan of PrefixLM</a> <a href="https://twitter.com/haileysch__/status/1805295308047282466?s=46">for a while</a>, mostly because we experimented with it a bit for BLOOMZ instruction tuning (See Appendix J)<d-cite key="muennighoff2023crosslingualgeneralizationmultitaskfinetuning"></d-cite> and wanted it to work, but it did not clearly make a difference in downstream performance, and certainly not enough to make it worth the added overhead and potential headache. However, I don’t think this means something like PrefixLM will never be worth it in the future!</p> <p>A very nice recent paper by Arora et al. (2024), JRT-RNN<d-cite key="arora2024justreadtwiceclosing"></d-cite>, actually presents a compelling case for PrefixLM being worth reevaluating for non-Transformer architectures. The authors justify their choice by showing that since optimized Linear Attention implementations are much faster than Flash Attention, they can get away with using the (typically more expensive) PrefixLM and still retain a significant speed advantage over a standard Transformer.</p> <p>But there’s actually a reason that PrefixLM with Linear Attention can counterintuitively be <em>even faster than full Causal LM</em>, even though for softmax attention it’d be more computation! The synergy goes even deeper. I suspect the JRT-RNN authors are already aware of this fact, but I wanted to write it out so others can understand this intuition.</p> <p>First, let’s briefly recap some of the disadvantages of PrefixLM for typical decoder-only GPT-style autoregressive LMs. If you are already familiar with PrefixLM and other LM objectives as well as their tradeoffs, feel free to skip to <a href="http://haileyschoelkopf.github.io/blog/2024/prefix-lm/#prefixlm-3-linear-attention">later sections</a>.</p> <h2 id="why-not-prefixlm-for-vanilla-transformers">Why Not PrefixLM For Vanilla Transformers?</h2> <h3 id="increased-costs">Increased Costs</h3> <p>Flash Attention<d-cite key="dao2022flashattentionfastmemoryefficientexact"></d-cite><d-cite key="dao2023flashattention2fasterattentionbetter"></d-cite>, by computing tiles of the \(L \times L\) attention matrix at a time, makes it possible to <em>skip tiles that are fully masked out</em>. This means we don’t have to compute anything where all positions in the tile are above the diagonal in the causal mask \(M\), and can get 2x faster attention when we’re doing fully-causal attention as compared to bidirectional attention.</p> <p>For PrefixLM, although we can take advantage of <em>some</em> of the masked-out tiles to speed up over the fully-bidirectional case<d-cite key="pytorch2024flexattention"></d-cite>, we still are forced to compute more values that wouldn’t be needed when performing fully causal attention. And so attention with PrefixLM will be unfortunately more costly.</p> <p>Also, when we’re training our model, typically we don’t get to compute any loss signal on the bidirectional-attention prefix tokens. This can reduce the number of tokens we learn from, which (might!) make training using PrefixLM less data-efficient than Causal LM.</p> <h3 id="extra-test-time-hyperparameter-tuning">Extra Test-Time Hyperparameter Tuning</h3> <p>PrefixLM also introduces a hyperparameter that must be fiddled with ($l \in [1, L]$ determining that $x_{\lt l}$ is treated as the bidirectional-attention “prefix”), either to get the best output quality, or to not push the model out-of-distribution from the typically fixed $l$ value used during training. This is not a problem Causal LM faces!</p> <h3 id="headaches-with-kv-caching">Headaches with KV Caching</h3> <p>If, when running our model for some use case like a multi-turn conversation, we wish to go back and change our value of $l$, then we must re-encode the entire input to create a new KV cache, rather than simply appending to the KV cache. This is an additional slowdown and cost that, again, Causal LM sidesteps.</p> <h3 id="unconvincing-empirical-improvements">Unconvincing Empirical Improvements</h3> <p>These issues would be ignorable if PrefixLM gave us much better models than Causal LM did.</p> <p>However, the empirical results in do not suggest this is the case<d-cite key="pmlr-v162-wang22u"></d-cite>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prefix-lm/prefix-lm-evals-480.webp 480w,/assets/img/prefix-lm/prefix-lm-evals-800.webp 800w,/assets/img/prefix-lm/prefix-lm-evals-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prefix-lm/prefix-lm-evals.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Zero-shot average results obtained by Wang et al. (2022). Among Causal Decoder (decoder-only model trained with Causal LM), Non-causal Decoder (decoder-only model trained with PrefixLM), and Encoder-Decoder (encoder-decoder model trained with PrefixLM), the Causal Decoder is the best zero-shot model. For our purposes, the main takeaway is that PrefixLM is no more performant than Causal LM, and can in fact underperform it.</figcaption> </figure> <p>A key caveat: These evals in Wang et al. (2022) are only <strong>loglikelihood-based classification</strong>, and are not especially high scores across the board at the scale tested in the paper. Papers like UL2R<d-cite key="tay2022transcendingscalinglaws01"></d-cite> have shown improvements from training or fine-tuning on PrefixLM or other non-causal objectives, although others anecdotally have had trouble reproducing this in their own codebases. It may be the case that either at larger scales, or on generative tasks / more subjective qualities, PrefixLM or other non-causal objectives lead to noticeable improvements–we as a community don’t currently know for sure.</p> <h2 id="prefixlm-3-linear-attention">PrefixLM &lt;3 Linear Attention</h2> <h3 id="recap-what-is-linear-attention">Recap: What is Linear Attention?</h3> <p>Standard Softmax attention is given by the following<d-footnote>We elide the scaling of $QK^T$ by $\frac{1}{\sqrt{d}}$ for simplicity.</d-footnote>:</p> \[O = \text{Softmax}\left(QK^T\right)V\] <p>where $Q, K, V \in \mathbb{R}^{L \times d}$.</p> <p>Linear attention makes this more efficient by replacing the Softmax function with a feature map \(\phi: \mathbb{R}^d \to \mathbb{R}^{d'}\) applied to Q and K separately:</p> \[O = \frac{(\phi(Q)\phi(K)^T) V}{\phi(Q)\sum^L_{i=1}\phi(K_i)^T}\] <p>where \(Q, K \in \mathbb{R}^{L \times d'}, V \in \mathbb{R}^{L \times d}\).</p> <p>This allows us to re-associate \((\phi(Q)\phi(K)^T) V\) as \(\phi(Q)(\phi(K)^T V)\), letting us avoid the \(O(L^2)\) complexity of softmax attention by never producing the intermediate \((\phi(Q)\phi(K)^T) \in \mathbb{R}^{L \times L}\).</p> <p>In practice it has been found that one can get away with avoiding the denominator terms<d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite>:</p> \[O = \phi(Q)(\phi(K)^T V)\] <p>so we’ll simplify Linear Attention to this going forward.</p> <h3 id="causality-and-chunked-algorithm">Causality and Chunked Algorithm</h3> <p>When we perform causal language modeling, we introduce a mask $M$ that prevents tokens from being affected by <em>future</em> tokens in the sequence:</p> \[O = \text{Softmax}\left(QK^T \odot M \right)V\] <p>However, when we introduce this causal mask $M$ into linear attention:</p> \[O = (\phi(Q)\phi(K)^T \odot M) V\] <p>We can no longer reorder the matmuls \((QK^T)V\) as \(Q(K^T V)\) freely! This forces us to not use the efficient form (\(O(Ldd')\)), and instead use a <em>chunked</em> form interpolating between purely-recurrent and purely-parallel forms:</p> <p>For each chunk<d-footnote>$[c]$ indicates the $c$-th "chunk", a.k.a. the span of indices from $[(c-1)C + 1, cC]$. $c$ can range from $1$ to $L//C$.</d-footnote>, we compute the starting state for that chunk, reusing the state computed for the chunk prior:</p> \[S_{[c+1]} = S_{[c]} + \sum_{i=((c-1)C + 1)}^{cC}\phi(k_i)^Tv_i = S_{[c]} + \phi(K_{[c]})^T V_{[c]}\] <p>and then to compute this chunk’s output $O_{[c]}$:</p> \[O_{[c+1]} = \phi(Q)_{[c+1]}S_{[c]} + (\phi(Q)_{[c+1]}\phi(K_{[c+1]})^T \odot M)V_{[c+1]}\] <p>we use the quadratic form (now quadratic in <em>chunk size</em>, not the entirety of $L$) while applying our causal mask $M$.</p> <p>This algorithm has \(O((L//C)(C^2d' + Cdd')) = O(LCd' + Ldd')\) complexity, times $L//C$ chunks. $C$ is a tunable parameter between $1$ and $L$ determining the chunk size $L // C$.</p> <h3 id="avoiding-chunking-with-prefixlm">Avoiding Chunking With PrefixLM</h3> <p>But for PrefixLM, the non-causal input component has no attention mask!<d-footnote>This may note always be the case, sadly, if we want to pack multiple documents together without allowing them to attend across documents.... but it will for some cases, like, say, prefilling a super-long input for a PrefixLM Linear Attention model.</d-footnote>. So we can freely compute in our more efficient ordering for that entire prefix component of the computation.</p> <p>This means that, on the entirely non-causal input (say, up to positions $\lt j$ ) we can use the naive \(O(Ldd')\) complexity algorithm to compute the final state all in one go! No need to tamp down quadratic complexity by chunking.</p> <p>We can simply compute our state $S_{\lt j}$ via</p> \[S_{\lt j} = \phi(K_{\lt j})^TV_{\lt j}\] <p><strong>Because we attend bidirectionally to all tokens $x_{\lt j}$, the state $S_{\lt j}$ is the singular state used for computing all outputs from the prefix!</strong>)</p> <p>Then, the output from our bidirectional linear attention is just:</p> \[O_{\lt j} = \phi(Q_{\lt j})(\phi(K_{\lt j})^TV_{\lt j}) = \phi(Q_{ \lt j})S_{\lt j}\] <p>Which is \(O(Ldd')\) complexity and consists of simply 2 matrix multipllies! So on the bidirectional components of PrefixLM, <em>it can be faster to compute bidirectional Prefix Linear Attention than its causal form!</em> This is in contrast to softmax attention, where PrefixLM requires strictly more computation than Causal LM.</p> <h3 id="caveats">Caveats</h3> <p>One major caveat of the speedup I’ve just explained is that many of the more recent and most performant linear attention variants use <em>data-dependent</em> state updates. This could hinder the usefulness of the previous observation. For example, in GLA, the original Linear Attention <em>time-invariant</em> state update rule:</p> \[S_t = S_{t-1} + k_{t}^Tv_{t}\] <p>becomes the <em>time-dependent</em> update rule</p> \[S_t = G_{t} \odot S_{t-1} + k_{t}^Tv_t\] <p>–based on some parameter calculated <em>as a function of the current input</em>, we decide how much to retain or “forget” the existing state when updating it with our new input. This is sometimes called different things like (data-dependent) “gating”, “decay”, or “selection” by various architectures.</p> <p>It’s not especially clear that one would want a data-dependent update like this when attending to inputs bidirectionally though–maybe we can get away with data-<em>independent</em> updates when encoding our prefix, but include this gating when attending causally to the output. Hydra<d-cite key="hwang2024hydrabidirectionalstatespace"></d-cite> may offer some hints here.</p> <h2 id="conclusion">Conclusion</h2> <p>In short–PrefixLM <em>can</em> sometimes grant a speedup over Causal LM for recurrent architectures, by removing the need to perform the <em>chunked</em> algorithm and directly computing the full input’s ending state! This flips the situation as compared to softmax attention, where PrefixLM is slower than Causal LM. So while it might be hard to justify for Transformers, it’s potentially easier to for Linear Attention!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thank you to Dan Goldstein for reading an early version of this blog post!</p>]]></content><author><name>Hailey Schoelkopf</name></author><category term="ML,"/><category term="Architectures,"/><category term="Linear-Attention,"/><category term="PrefixLM"/><summary type="html"><![CDATA[Notes on Prefix Language Modeling--and a surprising observation that PrefixLM can be *faster* than Causal LM under some architectural conditions.]]></summary></entry><entry><title type="html">Linear Attention Fundamentals</title><link href="https://haileyschoelkopf.github.io/blog/2024/linear-attn/" rel="alternate" type="text/html" title="Linear Attention Fundamentals"/><published>2024-07-09T00:00:00+00:00</published><updated>2024-07-09T00:00:00+00:00</updated><id>https://haileyschoelkopf.github.io/blog/2024/linear-attn</id><content type="html" xml:base="https://haileyschoelkopf.github.io/blog/2024/linear-attn/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post will be a short overview recapping key formulas and intuitions around the increasingly-popular family of methods under the umbrella of Linear Attention, first introduced by Katharopoulos et al. (2020) <d-cite key="katharopoulos2020transformersrnnsfastautoregressive"></d-cite>. The material in this post is also covered excellently by Yang, Wang et al. (2023)<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite> and Yang et al. (2024).</p> <p><em>This post will assume familiarity with the transformer architecture and softmax attention, and with KV caching for decoding.</em></p> <hr/> <p>The fundamental building block of the transformer is the scaled dot product / softmax self-attention, defined as the following:<d-footnote>We elide the scaling by $\sqrt d$ and the causal mask for simplicity, for now.</d-footnote></p> \[\text{Attention}(Q,K,V) = \text{Softmax}\left(QK^T\right)V\] <p>Softmax attention is very powerful, but is not especially friendly to fast inference, especially when operating on very long sequences: it requires \(O(L^2d)\) FLOP<d-footnote>We write "FLOP" to indicate a quantity of floating point operations and "FLOPs" to indicate "FLOP per second". </d-footnote> and (although circumvented by efficient, modern implementations such as Flash Attention<d-cite key="dao2022flashattentionfastmemoryefficientexact"></d-cite><d-cite key="dao2023flashattention2fasterattentionbetter"></d-cite>) naively \(O(L^2)\) memory.</p> <p>For this reason, many attempts have been made to recover softmax attention performance at \(&lt; O(L^2)\) complexity–for example, an operation that scales linearly in complexity as \(O(L)\) as sequence length increases. While many of the methods have commonalities or shared foundations, we’ll focus on one particular family of methods that fall under the header of “Linear Attentions”.</p> <h2 id="linearized-attention">Linear(ized) Attention</h2> <p>Katharopoulos et al. (2020) introduce the foundation of linear attention. We assume as input \(Q, K, V \in \mathbb{R}^{L \times d}\) . The core idea is that, as we can write scaled dot product / softmax attention as</p> \[\text{Softmax}\left(QK^T\right)V = \frac{ \text{exp}(QK^T) } {\sum_{i=1}^L \text{exp}(QK_i^T)} V\] <p>, we can view this as a special case of</p> \[\frac{ \text{sim}(Q, K) } {\sum_{i=1}^L \text{sim}(Q, K_i)} V\] <p>for any (non-negative) similarity function \(\text{sim}(\cdot, \cdot)\) between keys and queries, \(\text{sim}(Q, K)\) .</p> <p>Katharopoulos et al. note that this applies to any <em>kernel</em> \(\text{sim}: \mathbb{R}^d \times \mathbb{R}^{d'} \rightarrow \mathbb{R}_{+}\) (applied to each position in our sequence independently), where</p> \[\text{sim}(Q, K) = \phi(Q) \cdot \phi(K) = \phi(Q)\phi(K)^T\] <p>–that is, the similarity metric is induced by applying a “feature map” \(\phi\) independently to both Q and K. \(\phi\) can optionally change dimensionality of our queries and keys from $d$ to $d’$, though we will often see \(d' = d\) . \(\text{exp}()\) cannot be expressed exactly using any particular \(\phi\) <d-footnote>Although we can approximate it with polynomial-sized feature maps in the limit</d-footnote>, but a number of other useful functions including simply the identity can.</p> <p>Now that we’ve switched from \(\text{exp}(QK^T)\) to \(\text{sim}(Q, K) = \phi(Q)\phi(K)^T\) , we can leverage the fact that <strong>matrix multiplication is associative</strong> to rearrange our order of operations:</p> \[\frac{ \phi(Q)\phi(K)^T } {\sum_{i=1}^L \phi(Q)\phi(K_i)^T} V = \frac{ \phi(Q)(\phi(K)^T V)} {\phi(Q)\sum_{i=1}^L \phi(K_i)^T}\] <p>Now, instead of calculating \(\phi(Q)\phi(K)^T\) (size \(L \times L\) , and thus \(O(L^2)\) FLOP and memory)–we can first calculate \(\phi(K)^TV \in \mathbb{R}^{d' \times d}\) , and then calculate the product of \(\phi(Q) \in \mathbb{R}^{L \times d'}\) with \(\phi(K)^TV\) .</p> <p>This gives us a way to compute this kernel-based “linear attention” in \(O(Ldd')\) (typically \(O(Ld^2)\) in particular) FLOP and space–we are now <em>linear</em> in complexity with respect to sequence length!</p> <h2 id="recurrent-form">Recurrent Form</h2> <p>The above equations are perhaps easier to understand when written position-wise in terms of our outputs. Let \(O \in \mathbb{R}^{L \times d}\) be our attention mechanism output, and let \(o_l\) be the output at position \(l\) . Another caveat we have not yet dealt with is that GPT-style training requires a lower-triangular mask matrix \(M \in \mathbb{R}^{L \times L}\) to prevent attending to tokens that are in the “future” from our current one.</p> <p>We can then write our linear attention output for sequence position \(l\) , \(o_l\) , as</p> \[o_l = \frac{\phi(q_l)\sum_{i=1}^l \phi(k_i)^Tv_i}{\phi(q_l) \sum_{i=1}^l \phi(k_i)^T}\] <p>The equivalent for standard softmax attention is given by</p> \[o_l = \frac{\sum_{i=1}^l \text{exp}(q_lk_i^T)v_i}{\sum_{i=1}^l\text{exp}(q_lk_i^T)}\] <p>We can observe 2 characteristics here:</p> <ol> <li>At every timestep, softmax attention requires computing \(\text{exp}(q_lk_i^T)\) between our current query \(q_l\) and every prior key \(k_i\) . **A single decoding step at timestep \(l\) with softmax attention is \(O(l)\) complexity, because the transformer’s KV cache of past \(k_i,v_i\) for \(i \in [1, l]\) (its “state”) grows linearly with sequence length–as do the FLOP required!</li> <li>For linear attention, we must only compute a denominator \(\sum_{i=1}^l \phi(k_i)^T\) and \(\sum_{i=1}^l \phi(k_i)^Tv_i\) <em>*which are independent of the current query \(q_l\) **. This means we can *reuse</em> the previously computed values from up to step \(l-1\) !</li> </ol> <p>Specifically, if we let \(Z_{l-1} = \sum_{i=1}^{l-1} \phi(k_i)^T\) , and \(S_{l-1} = \sum_{i=1}^{l-1} \phi(k_i)^Tv_i\) , then</p> \[o_l = \frac{\phi(q_l) (S_{l-1} + \phi(k_l)^Tv_l)}{\phi(q_l)(Z_{l-1} + \phi(k_l)^T)}\] <p>Then, for calculating \(o_{l+1}\) , we can let \(S_l = (S_{l-1} + \phi(k_l)^Tv_l)\) and \(Z_l = (Z_{l-1} + \phi(k_l)^T)\) , and repeat this same calculation!</p> <p>This gives us a <em>recurrent view</em> of linear attention–if we maintain a constant-size <em>“state”</em> \(S_l\) and “normalizer” \(Z_l\) , we can perform each decoding timestep in \(O(1)\) time and memory! Katharopoulos et al. (2020) also show that this can be formally viewed as an RNN with matrix-valued state \(S_l \in \mathbb{R}^{d' \times d}\) .</p> <p>Some work <d-cite key="yang2024gatedlinearattentiontransformers"></d-cite><d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite> drops the \(Z_l\) normalizer due to numerical instabilities, and empirically doesn’t observe any problems. Additionally, using \(\phi\) equal to the identity also appears to work, interestingly <d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite>.</p> <p>This gives us a clean recurrent form for computing \(o_l\) from \(o_{l-1}\) :</p> \[S_l = S_{l-1} + k_l^Tv_l\] \[o_l = q_l S_l\] <p>Assuming Linear Attention is both 1) actually in practice faster to compute on hardware than softmax attention and 2) equally or nearly as performant downstream as the vanilla Softmax-attention transformer, this is very promising!</p> <h2 id="parallelization">Parallelization</h2> <p>The above <em>recurrent form</em> we’ve described is efficient at inference time, because we only have to make one update step and get to do this in fewer FLOP and lower memory overhead than in typical attention.</p> <p>However, when we want to actually train these models, we run into problems: computing \(S_l = S_{l-1} + k_l^Tv_l\) then \(o_l = q_l S_l\) via looping over our entire sequence length ( \(l \in [1, L]\) ) (full “recurrent mode”) is generally prohibitively slow, despite costing \(O(Ldd')\) FLOP. We are forced to compute each of our \(L\) timesteps sequentially (instead of in parallel as in softmax attention), and must save our potentially-large state \(S_l\) to memory and read it back from memory at each timestep.<d-footnote>Approaches like the custom kernel for performing a parallel scan in Mamba-1<d-cite key="gu2024mambalineartimesequencemodeling"></d-cite> can mitigate this by keeping the state in higher-bandwidth SRAM, but this imposes limitations on how large we can make our state without being forced to incur IO costs.</d-footnote></p> <p><strong>Does our linear attention permit a parallelizable form for training?</strong> Recall that in our original derivation, we wrote</p> \[O = \frac{ \phi(Q)(\phi(K)^T V)} {\phi(Q)\sum_{i=1}^L \phi(K_i)^T}\] <p>to indicate the computation of our entire output in parallel. However, when performing attention with a <em>causal mask</em> as done in GPT-style autoregressive language modeling, we are forced to compute</p> \[O = \frac{(\phi(Q)\phi(K)^T \odot M) V}{\phi(Q)\sum^L_{i=1}\phi(K_i)^T}\] <p>to obtain the right answer to avoid “seeing into the future”. This pointwise multiplication by our causal mask \(M \in \mathbb{R}^{L \times L}\) prevents us from using associativity to compute \(\phi(K)^TV\) first–losing the better complexity of linear attention we claimed. We need to compute \(\phi(Q)\phi(K)^T\) , requiring \(O(L^2)\) time.</p> <h3 id="chunkwise-form">Chunkwise Form</h3> <p>Luckily, not all is lost. Hua et al. (2022)<d-cite key="hua2022transformerqualitylineartime"></d-cite> propose a solution they term the “chunkwise parallel form” for linear attention training, which is later extended by others<d-cite key="yang2024fla"></d-cite><d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite> for even better efficiency.</p> <p>In particular, we can find a middle ground for efficient training of these models by striking the right balance between the \(O(L)\) recurrent and \(O(L^2)\) parallel forms. We can do this by performing computation in <em>chunks</em> across the sequence length, where we will use the parallel form for computing results within a chunk, and the recurrent form for transmitting information across chunks.</p> <p>We will split our sequence of length \(L\) into \(C\) chunks of length \(L // C\) . Following [RetNet<d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite>, Gated Linear Attention (GLA)<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite>, and DeltaNet<d-cite key="yang2024parallelizinglineartransformersdelta"></d-cite>] we will adopt the notation that \(\cdot_{[c]}\) denotes the given variable’s value for the \(c\) -th chunk.</p> <p>Now, we need to define a few components: first, our new update rule for going from the state \(S_{[c]}\) at the start of chunk \(c\) to the next chunk \(c+1\) ‘s starting state \(S_{[c+1]}\) is as follows:<d-footnote>As is convention in the rest of this post, we assume that chunk indices are 1-indexed: $c \in [1, L//C]$. So $S_{[1]}$ corresponds to tokens indexed between $[1, C]$.</d-footnote></p> \[S_{[c+1]} = S_{[c]} + \sum_{i=((c-1)C + 1)}^{cC}\phi(k_i)^Tv_i = S_{[c]} + \phi(K_{[c]})^T V_{[c]}\] <p>And to compute the output for chunk \(c+1\) , \(O_{[c+1]} \in \mathbb{R}^{C \times d}\) , we compute</p> \[O_{[c+1]} = \phi(Q)_{[c+1]}S_{[c]} + (\phi(Q)_{[c+1]}\phi(K_{[c+1]})^T \odot M)V_{[c+1]}\] <p>the first portion of the equation is the contribution across previous chunks, computed using our recurrent mode ( \(O(Cdd')\) ), while the latter term is the current chunk’s contribution to its output computed using the parallel mode ( \(O(C^2d')\) ).</p> <p>To compute the entire output \(O\) for all chunks, we have two options<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite><d-cite key="yang2024fla"></d-cite>:</p> <p>1) Precompute and <em>materialize</em> each chunk’s starting state: save \(S_{[c]}\) , \(\forall c \in [1, L//C]\) . This can be done by starting with \(S_{[0]} = O \in \mathbb{R}^{d' \times d}\), then sequentially calculating and storing \(S_{[c+1]} = S_{[c]} + \phi(K_{[c]})^T V_{[c]}\).</p> <p>2) Save no intermediate \(S_{[c]}\) aside from (optionally, during the prefill stage for inference) \(S_{L//C}\) , our final state.</p> <p>If we precompute and materialize our \(L//C\) per-chunk starting states, then we can <strong>calculate all \(O_{[c]}\) simultaneously</strong>, since each \(O_{[c+1]}\) depends only on \(S_{[c]}\) . During training, we can also maintain these per-chunk states in order to more quickly perform the backward pass. However, we do pay a memory and IO cost: we must store \(C\) chunks of size \(d' \times d\) , resulting in \(O(Cdd')\) memory overhead.</p> <p>Alternately, we can avoid materializing any states \(S_{[c]}\) . This will force us to compute each \(O_{[c+1]}\) sequentially: for \(c \in [1, L//C]\) , once \(S_{[c-1]}\) has been computed, we can calculate \(O_{[c]}\) , and subsequently update \(S_{[c-1]}\) to \(S_{[c]}\) using our chunkwise update rule and subsequently compute \(O_{[c+1]}\) , and so on, until we have computed our full output \(O\) across all chunks. We pay no memory overhead due to not storing any intermediate states, but in the backward pass we will have to recompute these per-chunk states, requiring extra FLOP.</p> <p>This chunkwise formulation allows us to interpolate between the parallel and recurrent forms, choosing \(C\) based on which is fastest, and ends up being faster than full recurrence because we can take advantage of fast matrix multiplications without paying a cost quadratic in \(L\) !</p> <p>This chunkwise formulation is also adopted by the Mamba-2 / SSD<d-cite key="dao2024transformersssmsgeneralizedmodels"></d-cite> architecture–chunkwise parallelism is very hardware-friendly. This chunked algorithm is sometimes called “Flash Linear Attention” for this reason <d-cite key="yang2024gatedlinearattentiontransformers"></d-cite><d-cite key="yang2024fla"></d-cite>.</p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we’ve seen:</p> <ul> <li>How “linear attention” is derived and originally motivated</li> <li>How this can be viewed as an RNN with a matrix-valued state</li> <li>How to make training these theoretically-linear-complexity models efficient on hardware</li> </ul> <p>Again, this post is based off of the excellent exposition in Gated Linear Attention<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite>, Parallel DeltaNet<d-cite key="yang2024parallelizinglineartransformersdelta"></d-cite>, and the original Linear Attention paper. If you’re interested in this topic, I’d highly recommend checking them out as a reference point!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thanks to Arun Kumar for reading an early version of this blog post!</p>]]></content><author><name>Hailey Schoelkopf</name></author><category term="ML,"/><category term="Architectures,"/><category term="Linear-Attention"/><summary type="html"><![CDATA[The basics of linear attention in sub-quadratic language model architectures.]]></summary></entry></feed>