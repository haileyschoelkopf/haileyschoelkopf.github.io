<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Attention Fundamentals | Hailey Schoelkopf </title> <meta name="author" content="Hailey Schoelkopf"> <meta name="description" content="The basics of linear attention in sub-quadratic language model architectures."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://haileyschoelkopf.github.io/blog/2024/linear-attn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Linear Attention Fundamentals",
            "description": "The basics of linear attention in sub-quadratic language model architectures.",
            "published": "July 09, 2024",
            "authors": [
              
              {
                "author": "Hailey Schoelkopf",
                "authorURL": "https://haileyschoelkopf.github.io",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hailey</span> Schoelkopf </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Linear Attention Fundamentals</h1> <p>The basics of linear attention in sub-quadratic language model architectures.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#linear-ized-attention">Linear(ized) Attention</a> </div> <div> <a href="#recurrent-form">Recurrent Form</a> </div> <div> <a href="#parallelization">Parallelization</a> </div> <ul> <li> <a href="#chunkwise-form">Chunkwise Form</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>This post will be a short overview recapping key formulas and intuitions around the increasingly-popular family of methods under the umbrella of Linear Attention, first introduced by Katharopoulos et al. (2020) <d-cite key="katharopoulos2020transformersrnnsfastautoregressive"></d-cite>. The material in this post is also covered excellently by Yang, Wang et al. (2023)<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite> and Yang et al. (2024).</p> <p><em>This post will assume familiarity with the transformer architecture and softmax attention, and with KV caching for decoding.</em></p> <hr> <p>The fundamental building block of the transformer is the scaled dot product / softmax self-attention, defined as the following:<d-footnote>We elide the scaling by $\sqrt d$ and the causal mask for simplicity, for now.</d-footnote></p> \[\text{Attention}(Q,K,V) = \text{Softmax}\left(QK^T\right)V\] <p>Softmax attention is very powerful, but is not especially friendly to fast inference, especially when operating on very long sequences: it requires \(O(L^2d)\) FLOP<d-footnote>We write "FLOP" to indicate a quantity of floating point operations and "FLOPs" to indicate "FLOP per second". </d-footnote> and (although circumvented by efficient, modern implementations such as Flash Attention<d-cite key="dao2022flashattentionfastmemoryefficientexact"></d-cite><d-cite key="dao2023flashattention2fasterattentionbetter"></d-cite>) naively \(O(L^2)\) memory.</p> <p>For this reason, many attempts have been made to recover softmax attention performance at \(&lt; O(L^2)\) complexity–for example, an operation that scales linearly in complexity as \(O(L)\) as sequence length increases. While many of the methods have commonalities or shared foundations, we’ll focus on one particular family of methods that fall under the header of “Linear Attentions”.</p> <h2 id="linearized-attention">Linear(ized) Attention</h2> <p>Katharopoulos et al. (2020) introduce the foundation of linear attention. We assume as input \(Q, K, V \in \mathbb{R}^{L \times d}\) . The core idea is that, as we can write scaled dot product / softmax attention as</p> \[\text{Softmax}\left(QK^T\right)V = \frac{ \text{exp}(QK^T) } {\sum_{i=1}^L \text{exp}(QK_i^T)} V\] <p>, we can view this as a special case of</p> \[\frac{ \text{sim}(Q, K) } {\sum_{i=1}^L \text{sim}(Q, K_i)} V\] <p>for any (non-negative) similarity function \(\text{sim}(\cdot, \cdot)\) between keys and queries, \(\text{sim}(Q, K)\) .</p> <p>Katharopoulos et al. note that this applies to any <em>kernel</em> \(\text{sim}: \mathbb{R}^d \times \mathbb{R}^{d'} \rightarrow \mathbb{R}_{+}\) (applied to each position in our sequence independently), where</p> \[\text{sim}(Q, K) = \phi(Q) \cdot \phi(K) = \phi(Q)\phi(K)^T\] <p>–that is, the similarity metric is induced by applying a “feature map” \(\phi\) independently to both Q and K. \(\phi\) can optionally change dimensionality of our queries and keys from $d$ to $d’$, though we will often see \(d' = d\) . \(\text{exp}()\) cannot be expressed exactly using any particular \(\phi\) <d-footnote>Although we can approximate it with polynomial-sized feature maps in the limit</d-footnote>, but a number of other useful functions including simply the identity can.</p> <p>Now that we’ve switched from \(\text{exp}(QK^T)\) to \(\text{sim}(Q, K) = \phi(Q)\phi(K)^T\) , we can leverage the fact that <strong>matrix multiplication is associative</strong> to rearrange our order of operations:</p> \[\frac{ \phi(Q)\phi(K)^T } {\sum_{i=1}^L \phi(Q)\phi(K_i)^T} V = \frac{ \phi(Q)(\phi(K)^T V)} {\phi(Q)\sum_{i=1}^L \phi(K_i)^T}\] <p>Now, instead of calculating \(\phi(Q)\phi(K)^T\) (size \(L \times L\) , and thus \(O(L^2)\) FLOP and memory)–we can first calculate \(\phi(K)^TV \in \mathbb{R}^{d' \times d}\) , and then calculate the product of \(\phi(Q) \in \mathbb{R}^{L \times d'}\) with \(\phi(K)^TV\) .</p> <p>This gives us a way to compute this kernel-based “linear attention” in \(O(Ldd')\) (typically \(O(Ld^2)\) in particular) FLOP and space–we are now <em>linear</em> in complexity with respect to sequence length!</p> <h2 id="recurrent-form">Recurrent Form</h2> <p>The above equations are perhaps easier to understand when written position-wise in terms of our outputs. Let \(O \in \mathbb{R}^{L \times d}\) be our attention mechanism output, and let \(o_l\) be the output at position \(l\) . Another caveat we have not yet dealt with is that GPT-style training requires a lower-triangular mask matrix \(M \in \mathbb{R}^{L \times L}\) to prevent attending to tokens that are in the “future” from our current one.</p> <p>We can then write our linear attention output for sequence position \(l\) , \(o_l\) , as</p> \[o_l = \frac{\phi(q_l)\sum_{i=1}^l \phi(k_i)^Tv_i}{\phi(q_l) \sum_{i=1}^l \phi(k_i)^T}\] <p>The equivalent for standard softmax attention is given by</p> \[o_l = \frac{\sum_{i=1}^l \text{exp}(q_lk_i^T)v_i}{\sum_{i=1}^l\text{exp}(q_lk_i^T)}\] <p>We can observe 2 characteristics here:</p> <ol> <li>At every timestep, softmax attention requires computing \(\text{exp}(q_lk_i^T)\) between our current query \(q_l\) and every prior key \(k_i\) . **A single decoding step at timestep \(l\) with softmax attention is \(O(l)\) complexity, because the transformer’s KV cache of past \(k_i,v_i\) for \(i \in [1, l]\) (its “state”) grows linearly with sequence length–as do the FLOP required!</li> <li>For linear attention, we must only compute a denominator \(\sum_{i=1}^l \phi(k_i)^T\) and \(\sum_{i=1}^l \phi(k_i)^Tv_i\) <em>*which are independent of the current query \(q_l\) **. This means we can *reuse</em> the previously computed values from up to step \(l-1\) !</li> </ol> <p>Specifically, if we let \(Z_{l-1} = \sum_{i=1}^{l-1} \phi(k_i)^T\) , and \(S_{l-1} = \sum_{i=1}^{l-1} \phi(k_i)^Tv_i\) , then</p> \[o_l = \frac{\phi(q_l) (S_{l-1} + \phi(k_l)^Tv_l)}{\phi(q_l)(Z_{l-1} + \phi(k_l)^T)}\] <p>Then, for calculating \(o_{l+1}\) , we can let \(S_l = (S_{l-1} + \phi(k_l)^Tv_l)\) and \(Z_l = (Z_{l-1} + \phi(k_l)^T)\) , and repeat this same calculation!</p> <p>This gives us a <em>recurrent view</em> of linear attention–if we maintain a constant-size <em>“state”</em> \(S_l\) and “normalizer” \(Z_l\) , we can perform each decoding timestep in \(O(1)\) time and memory! Katharopoulos et al. (2020) also show that this can be formally viewed as an RNN with matrix-valued state \(S_l \in \mathbb{R}^{d' \times d}\) .</p> <p>Some work <d-cite key="yang2024gatedlinearattentiontransformers"></d-cite><d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite> drops the \(Z_l\) normalizer due to numerical instabilities, and empirically doesn’t observe any problems. Additionally, using \(\phi\) equal to the identity also appears to work, interestingly <d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite>.</p> <p>This gives us a clean recurrent form for computing \(o_l\) from \(o_{l-1}\) :</p> \[S_l = S_{l-1} + k_l^Tv_l\] \[o_l = q_l S_l\] <p>Assuming Linear Attention is both 1) actually in practice faster to compute on hardware than softmax attention and 2) equally or nearly as performant downstream as the vanilla Softmax-attention transformer, this is very promising!</p> <h2 id="parallelization">Parallelization</h2> <p>The above <em>recurrent form</em> we’ve described is efficient at inference time, because we only have to make one update step and get to do this in fewer FLOP and lower memory overhead than in typical attention.</p> <p>However, when we want to actually train these models, we run into problems: computing \(S_l = S_{l-1} + k_l^Tv_l\) then \(o_l = q_l S_l\) via looping over our entire sequence length ( \(l \in [1, L]\) ) (full “recurrent mode”) is generally prohibitively slow, despite costing \(O(Ldd')\) FLOP. We are forced to compute each of our \(L\) timesteps sequentially (instead of in parallel as in softmax attention), and must save our potentially-large state \(S_l\) to memory and read it back from memory at each timestep.<d-footnote>Approaches like the custom kernel for performing a parallel scan in Mamba-1<d-cite key="gu2024mambalineartimesequencemodeling"></d-cite> can mitigate this by keeping the state in higher-bandwidth SRAM, but this imposes limitations on how large we can make our state without being forced to incur IO costs.</d-footnote></p> <p><strong>Does our linear attention permit a parallelizable form for training?</strong> Recall that in our original derivation, we wrote</p> \[O = \frac{ \phi(Q)(\phi(K)^T V)} {\phi(Q)\sum_{i=1}^L \phi(K_i)^T}\] <p>to indicate the computation of our entire output in parallel. However, when performing attention with a <em>causal mask</em> as done in GPT-style autoregressive language modeling, we are forced to compute</p> \[O = \frac{(\phi(Q)\phi(K)^T \odot M) V}{\phi(Q)\sum^L_{i=1}\phi(K_i)^T}\] <p>to obtain the right answer to avoid “seeing into the future”. This pointwise multiplication by our causal mask \(M \in \mathbb{R}^{L \times L}\) prevents us from using associativity to compute \(\phi(K)^TV\) first–losing the better complexity of linear attention we claimed. We need to compute \(\phi(Q)\phi(K)^T\) , requiring \(O(L^2)\) time.</p> <h3 id="chunkwise-form">Chunkwise Form</h3> <p>Luckily, not all is lost. Hua et al. (2022)<d-cite key="hua2022transformerqualitylineartime"></d-cite> propose a solution they term the “chunkwise parallel form” for linear attention training, which is later extended by others<d-cite key="yang2024fla"></d-cite><d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite> for even better efficiency.</p> <p>In particular, we can find a middle ground for efficient training of these models by striking the right balance between the \(O(L)\) recurrent and \(O(L^2)\) parallel forms. We can do this by performing computation in <em>chunks</em> across the sequence length, where we will use the parallel form for computing results within a chunk, and the recurrent form for transmitting information across chunks.</p> <p>We will split our sequence of length \(L\) into \(C\) chunks of length \(L // C\) . Following [RetNet<d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite>, Gated Linear Attention (GLA)<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite>, and DeltaNet<d-cite key="yang2024parallelizinglineartransformersdelta"></d-cite>] we will adopt the notation that \(\cdot_{[c]}\) denotes the given variable’s value for the \(c\) -th chunk.</p> <p>Now, we need to define a few components: first, our new update rule for going from the state \(S_{[c]}\) at the start of chunk \(c\) to the next chunk \(c+1\) ‘s starting state \(S_{[c+1]}\) is as follows:<d-footnote>As is convention in the rest of this post, we assume that chunk indices are 1-indexed: $c \in [1, L//C]$. So $S_{[1]}$ corresponds to tokens indexed between $[1, C]$.</d-footnote></p> \[S_{[c+1]} = S_{[c]} + \sum_{i=((c-1)C + 1)}^{cC}\phi(k_i)^Tv_i = S_{[c]} + \phi(K_{[c]})^T V_{[c]}\] <p>And to compute the output for chunk \(c+1\) , \(O_{[c+1]} \in \mathbb{R}^{C \times d}\) , we compute</p> \[O_{[c+1]} = \phi(Q)_{[c+1]}S_{[c]} + (\phi(Q)_{[c+1]}\phi(K_{[c+1]})^T \odot M)V_{[c+1]}\] <p>the first portion of the equation is the contribution across previous chunks, computed using our recurrent mode ( \(O(Cdd')\) ), while the latter term is the current chunk’s contribution to its output computed using the parallel mode ( \(O(C^2d')\) ).</p> <p>To compute the entire output \(O\) for all chunks, we have two options<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite><d-cite key="yang2024fla"></d-cite>:</p> <p>1) Precompute and <em>materialize</em> each chunk’s starting state: save \(S_{[c]}\) , \(\forall c \in [1, L//C]\) . This can be done by starting with \(S_{[0]} = O \in \mathbb{R}^{d' \times d}\), then sequentially calculating and storing \(S_{[c+1]} = S_{[c]} + \phi(K_{[c]})^T V_{[c]}\).</p> <p>2) Save no intermediate \(S_{[c]}\) aside from (optionally, during the prefill stage for inference) \(S_{L//C}\) , our final state.</p> <p>If we precompute and materialize our \(L//C\) per-chunk starting states, then we can <strong>calculate all \(O_{[c]}\) simultaneously</strong>, since each \(O_{[c+1]}\) depends only on \(S_{[c]}\) . During training, we can also maintain these per-chunk states in order to more quickly perform the backward pass. However, we do pay a memory and IO cost: we must store \(C\) chunks of size \(d' \times d\) , resulting in \(O(Cdd')\) memory overhead.</p> <p>Alternately, we can avoid materializing any states \(S_{[c]}\) . This will force us to compute each \(O_{[c+1]}\) sequentially: for \(c \in [1, L//C]\) , once \(S_{[c-1]}\) has been computed, we can calculate \(O_{[c]}\) , and subsequently update \(S_{[c-1]}\) to \(S_{[c]}\) using our chunkwise update rule and subsequently compute \(O_{[c+1]}\) , and so on, until we have computed our full output \(O\) across all chunks. We pay no memory overhead due to not storing any intermediate states, but in the backward pass we will have to recompute these per-chunk states, requiring extra FLOP.</p> <p>This chunkwise formulation allows us to interpolate between the parallel and recurrent forms, choosing \(C\) based on which is fastest, and ends up being faster than full recurrence because we can take advantage of fast matrix multiplications without paying a cost quadratic in \(L\) !</p> <p>This chunkwise formulation is also adopted by the Mamba-2 / SSD<d-cite key="dao2024transformersssmsgeneralizedmodels"></d-cite> architecture–chunkwise parallelism is very hardware-friendly. This chunked algorithm is sometimes called “Flash Linear Attention” for this reason <d-cite key="yang2024gatedlinearattentiontransformers"></d-cite><d-cite key="yang2024fla"></d-cite>.</p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we’ve seen:</p> <ul> <li>How “linear attention” is derived and originally motivated</li> <li>How this can be viewed as an RNN with a matrix-valued state</li> <li>How to make training these theoretically-linear-complexity models efficient on hardware</li> </ul> <p>Again, this post is based off of the excellent exposition in Gated Linear Attention<d-cite key="yang2024gatedlinearattentiontransformers"></d-cite>, Parallel DeltaNet<d-cite key="yang2024parallelizinglineartransformersdelta"></d-cite>, and the original Linear Attention paper. If you’re interested in this topic, I’d highly recommend checking them out as a reference point!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thanks to Arun Kumar for reading an early version of this blog post!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-07-09-linear-attn.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hailey Schoelkopf. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 13, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>